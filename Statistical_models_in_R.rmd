---
title: '**Solving real-world problems with statistical modelling**'
author: '*Cynthia Thinwa*'
output:
  pdf_document:
    df_print: kable
---

# Problem 1

As part of a larger case-control study, an investigator decided to identify factors associated with giving birth to a low birth weight baby (weighing less than 2500 grams).

Women were matched according to age so that each case was matched to one control.

The dataset is **lowbwtmat.csv** and it contains the following variables:

Variable | Code/Value | Abbreviation
-------- | ---------- | ------------
Identification Code | $~~$ | ID
$~~$ | $~~$ | $~~$
Low Birth Weight | 0= if weight is >=2500g | LOW
$~~$ | 1=if weight is < 2500g | $~~$
$~~$ | $~~$ | $~~$
Mother’s weight at last menstrual | $~~$   | LWT
$~~$ | $~~$ | $~~$
Smoking during pregnancy | 0=No | SMOKE
$~~$ | 1=Yes | $~~$
$~~$ | $~~$ | $~~$
History of premature labour | 0=No | PTL
$~~$ | 1=Yes | $~~$
$~~$ | $~~$ | $~~$  
History of hypertension | 0=No | HT
$~~$ | 1=Yes | $~~$
$~~$ | $~~$ | $~~$  
Urinary tract infection | 0=No | UI
$~~$ | 1=Yes | $~~$
$~~$ | $~~$ | $~~$  
Physician Visit during the first trimester | 0=No | FTV
$~~$ | 1=Yes | $~~$

(i) For each variable given except for the outcome,
    state whether it is:
    
    * a predictor,
    * a confounder, 
    * an effect modifier or
    * none of the above.
    
    Give reason for your answer.
    
(ii) Fit the appropriate model based on part (i) and give a brief report of the analysis.

$~~$

## Solution for Question 1

### (i)

Variable | Correct Category | Reason
-------- | ------- | ---------------
Identification Code |  None | The variable is simply for identification purposes.
$~~$ | $~~$ | $~~$
Low Birth Weight | None | This is the variable that is of interest (the dependent variable), and is an outcome in and of itself.
$~~$ | $~~$ | $~~$
Mother’s weight at last menstrual | Confounder | This could be tied to the baby's birth weight in some way, but it is not directly related with the dependent variable.
$~~$ | $~~$ | $~~$
Smoking during pregnancy | Predictor | This variable has a significant effect on the baby during pregnancy, therefore it is linked to the baby's birth weight.
$~~$ | $~~$ | $~~$
History of premature labour | Confounder | Premature birth often results in a baby's birth weight being low, therefore this history could indicate that a premature birth could occur.
$~~$ | $~~$ | $~~$  
History of hypertension | Predictor | A history of hypertension could indicate that the baby's development was affected by hypertension, therefore it is linked to the baby's birth weight.
$~~$ | $~~$ | $~~$  
Urinary tract infection | Confounder | This could affect delivery of the baby, but it is not directly related to the dependent variable.
$~~$ | $~~$ | $~~$  
Physician Visit during the first trimester | Effect modifier | This variable has a significant effect on the baby during pregnancy, therefore it is linked to the baby's birth weight.

### (ii)

An appropriate model to fit the data is the binary logistic regression model because the dependent variable has two categories. Stepwise regression based on the AIC was conducted, resulting in the following models:

```{r Q1data & bor, echo=FALSE}
library(MASS)

lowbwtmat <- read.csv('lowbwtmat.csv')

low.birth.weight <- factor(lowbwtmat$LOW)

mothers.weight.last.menstrual <- lowbwtmat$LWT

smoked <- factor(lowbwtmat$SMOKE)

history.premature.labour <- factor(lowbwtmat$PTL)

history.hypertension <- factor(lowbwtmat$HT)

UTI <- factor(lowbwtmat$UI)

doc.visit.first.trimester <- factor(lowbwtmat$FTV)

Question1Model <- glm(low.birth.weight ~ mothers.weight.last.menstrual
                      + smoked + history.premature.labour + history.hypertension
                      + UTI + doc.visit.first.trimester,
                      family = binomial) # the full model

Question1Model_null <- glm(low.birth.weight~1, family = binomial) # the null model

step(Question1Model_null, direction = 'both', scope = formula(Question1Model))
```

$~~$

A more detailed description of the best model findings were as follows:

```{r Q1bestmodel, echo=FALSE}
Q1BestModel <- glm(formula = low.birth.weight ~ smoked + history.premature.labour + 
    UTI + history.hypertension + mothers.weight.last.menstrual, 
    family = binomial); summary(Q1BestModel)
```


$~~$

#### Statistical significance of the model

$~~$

A Likelihood Ratio Test was conducted and the results were as follows:

```{r Q1 lrtest, echo=FALSE}
Question1Model_null <- glm(low.birth.weight~1, family = binomial) # the null model
anova(Question1Model_null, Q1BestModel, test="LRT")
```

Based on the results, the model is statistically significant because the p-value associated with the test statistic is << 0.05

$~~$

#### Statistical significance of the coefficients

$~~$

The mother's weight at her last menstrual, smoking during pregnancy and the mother having a history of hypertension were found to be statistically significant because their p-values were < 0.05.

However, the remaining variables were found to not be statistically significant because their p-values were > 0.05.

$~~$

#### Interpretation of parameter estimates

$~~$

The adjusted odds ratios for the intercept and coefficients with a 95% confidence interval were as follows:

```{r Q1oratios, echo=FALSE, message=FALSE}
Q1O.R.results <- cbind(exp(coef(Q1BestModel)), exp(confint(Q1BestModel)))

Q1O.R.results <- round(Q1O.R.results, 4)

Q1O.R.results <- as.data.frame(Q1O.R.results)

colnames(Q1O.R.results) <- c("O.R.", "2.5 %", "97.5 %"); Q1O.R.results
```

1.  **Smoking during pregnancy**: Adjusting for all other factors, babies born to mothers who smoked during pregnancy are 3 times more likely to have a low birth weight than babies born to mothers who did not smoke during pregnancy.

2.  **History of premature labour**: Adjusting for all other factors, babies born to mothers with a history of premature labour are 3 times more likely to have a low birth weight than babies born to mothers with no history of premature labour.

3.  **Urinary Tract Infection**: Adjusting for all other factors, babies born to mothers having a urinary tract infection are 3 times more likely to have a low birth weight than babies born to mothers not having a urinary tract infection.

4.  **History of hypertension**: Adjusting for all other factors, babies born to mothers with a history of hypertension are 6 times more likely to have a low birth weight than babies born to mothers with no history of hypertension.

5.  **Mother's weight at their last menstrual**: For every unit increase in the mother's weight at their last menstrual holding all other variables constant, babies are 1.6% less likely to have a low birth weight.

$$~~$$

# Problem 2

Researchers in a certain county tracked flu cases requiring hospitalization in those residents aged 65 and older during a two-month period one winter. 

They matched each case with 2 controls by sex and age (150 cases, 300 controls). They used medical records to determine whether cases and controls had received a flu vaccine shot and whether they had underlying lung disease.

Their interest was to determine if flu vaccination prevents hospitalization for flu (severe cases of flu). The underlying lung disease is a potential confounder. The dataset is **flumatch.csv** and variables are:

Variable | Code
-------- | ----------------------
Outcome | 0 = Control
$~~$ | 1 = Case
$~~$ | $~~$
Vaccine | 0 = not vaccinated
$~~$ | 1 = vaccinated
$~~$ | $~~$ | $~~$
Lung | 0 = no underlying lung disease
$~~$ | 1 = underlying lung disease
$~~$ | $~~$ | $~~$
Id | Identifier for each matching group (1 case, 2 controls)

What conclusion do you think the researchers made?

$~~$

## Solution for Question 2

We are trying to see if flu infection among the elderly in wintertime has a relationship with flu vaccination and underlying lung disease.

Therefore, a binary regression model will be useful to help us achieve this goal.

Let $Y = outcome$ where if the individual is a patient, $Y=1$ and if they are not a patient, $Y=0$

Let $X_1 = vaccine$ (a vaccination for the flu) and $X_2 = lungdisease$ (underlying lung disease)

Stepwise regression based on the AIC was conducted, resulting in the following models:

```{r Q2data, echo=FALSE}
flumatch <- read.csv('flumatch.csv')

flu.infection <- factor(flumatch$outcome)

vaccinated <- factor(flumatch$vaccine)

has.lung.disease <- factor(flumatch$lungdisease)
```

```{r Q2bor, echo=FALSE}
Question2Model <- glm(flu.infection~vaccinated+has.lung.disease, family = binomial) # the full model

Question2Model_null <- glm(flu.infection~1, family = binomial) # the null model

step(Question2Model_null, direction = 'both', scope = formula(Question2Model))
```

$~~$

A more detailed description of the best model findings were as follows:

```{r Q2bestmodel, echo=FALSE}
summary(Question2Model)
```

$$~~$$

$$~~$$

### Statistical significance of the model

The findings of the Likelihood Ratio Test were as follows:

```{r Q2lrtest, echo=FALSE}
Question2Model_null <- glm(flu.infection~1, family = binomial) # the null model
anova(Question2Model_null, Question2Model, test="LRT")
```

Based on the results, the model is statistically significant because the p-value is << 0.05

$~~$

### Statistical significance of the coefficients

The vaccination variable does not appear to have a statistically significant relationship with the dependent variable because its p-value is > 0.05.

However, the underlying lung disease variable has a statistically significant relationship with the dependent variable because its p-value is << 0.05.

$~~$

### Interpretation of parameter estimates

The adjusted odds ratios for the intercept and coefficients with a 95% confidence interval were as follows:

```{r Q2oratios, echo=FALSE, message=FALSE}
Q2O.R.results <- cbind(exp(coef(Question2Model)), exp(confint(Question2Model)))

Q2O.R.results <- round(Q2O.R.results, 4)

Q2O.R.results <- as.data.frame(Q2O.R.results)

colnames(Q2O.R.results) <- c("O.R.", "2.5 %", "97.5 %"); Q2O.R.results
```


1.  **Flu vaccination**: Adjusting for all other factors, people who have received a flu vaccination are 61.05% less likely to get infected with the flu than people who are not vaccinated against the flu.

2. **Underlying lung disease**: Adjusting for all other factors, people who have underlying lung disease are 4 times more likely to get infected with the flu than people who are not vaccinated against the flu.

$~~$

### Conclusion

The relationship between flu infection and underlying lung disease should be studied in more depth.

$$~~$$

$$~~$$

$$~~$$

$$~~$$

# Problem 3

The data set **serv.csv** gives part of data obtained during a 10 year follow up study on risk factors associated with death due to cancer for those serving in the military in Britain. 

The number of deaths are recorded per person years for the pair combination of type of service(veteran or non veteran) and age category of the soldiers.

Are the two factors significantly associated with cancer deaths?

$~~$

## Solution for Question 3

When count data contains unequal time periods, the response variable should be the average number of events per unit time.

Therefore, the data around this question is as follows:

```{r Q3data, echo=FALSE}
serv <- read.csv('serv.csv')

serv$service <- as.character(serv$service)

serv$service[12] <- 'non-veteran'

colnames(serv) <- c("cancer", "person.yrs", "age", "service")

serv$observed.rate <- serv$cancer/serv$person.yrs

serv$mean.cancer.per.100000.pyrs <- round(serv$observed.rate*100000,0); serv
```

$~~$

Because we are dealing with count data over a given period of time, we can apply the Poisson regression model provided that the mean and variance in the count data are as close as possible.

The mean and variance for our observed rates is as follows:

```{r Q3PoissonTest, echo=FALSE}
print('Mean of the observed rates:')
mean(serv$mean.cancer.per.100000.pyrs)
print('Variance of the observed rates:')
var(serv$mean.cancer.per.100000.pyrs)
```

Because the mean and the variance differ significantly, a negative binomial regression model will be used as it is useful for count data that has a significant difference between its mean and its variance.

Stepwise regression based on the AIC was conducted, resulting in the following models:

```{r Q3 negbi, echo=FALSE, warning=FALSE, message=FALSE}
library(MASS)

mean.number.of.cancer.cases <- serv$mean.cancer.per.100000.pyrs

age.group <- factor(serv$age)

service <- factor(serv$service)

Question3Model <- glm.nb(mean.number.of.cancer.cases~age.group+service) # the full model

Question3Model_null <- glm.nb(mean.number.of.cancer.cases~1) # the null model

step(Question3Model_null, direction = 'both', scope = formula(Question3Model))
```

$~~$

A more detailed description of the best model findings were as follows:

```{r Q3bestmodel, echo=FALSE, message=FALSE, warning=FALSE}
Q3BestModel <- glm.nb(formula = mean.number.of.cancer.cases ~ age.group, init.theta = 3890101.522, link = log); summary(Q3BestModel)
```

$~~$

### Statistical significance of the model

A Likelihood Ratio test was conducted and the results were as follows:

```{r Q3lrtest, echo=FALSE, message=FALSE, warning=FALSE}
anova(Question3Model_null, Q3BestModel, test="Chisq")
```

Based on the results, the model is statistically significant because the p-value is << 0.05.

$~~$

### Statistical significance of the coefficients

**Age**: With the age group of 0-24 as the reference category, all dummy variables formed from the age variable except the 25-29 age group dummy variable were found to be statistically significant because their p-values were << 0.05.

$~~$

### Interpretation of parameter estimates


The adjusted odds ratio for the intercept and coefficients with a 95% confidence interval were as follows:

```{r Q3oratios, echo=FALSE, message=FALSE}
Q3O.R.results <- cbind(exp(coef(Q3BestModel)), exp(confint(Q3BestModel)))

Q3O.R.results <- round(Q3O.R.results, 4)

Q3O.R.results <- as.data.frame(Q3O.R.results)

colnames(Q3O.R.results) <- c("O.R.", "2.5 %", "97.5 %"); Q3O.R.results
```

**Age**
   
The findings for the different categories were as follows:
   
i)    People whose ages fall in the 25-29 category were 73.68% more likely to contract cancer compared to those in the 0-24 age category.

ii)   People whose ages fall in the 30-34 category were 4 times more likely to contract cancer compared to those in the 0-24 age category.

iii)  People whose ages fall in the 35-39 category were 7 times more likely to contract cancer compared to those in the 0-24 age category.

iv)   People whose ages fall in the 40-44 category were 7 times more likely to contract cancer compared to those in the 0-24 age category.

v)  People whose ages fall in the 45-39 category were 14 times more likely to contract cancer compared to those in the 0-24 age category.

vi)   People whose ages fall in the 50-54 category were 23 times more likely to contract cancer compared to those in the 0-24 age category.

vii)  People whose ages fall in the 55-59 category were 39 times more likely to contract cancer compared to those in the 0-24 age category.

viii)   People whose ages fall in the 60-64 category were 57 times more likely to contract cancer compared to those in the 0-24 age category.

ix)  People whose ages fall in the 65-69 category were 63 times more likely to contract cancer compared to those in the 0-24 age category.

x)  People whose ages fall in the 70+ category were 75 times more likely to contract cancer compared to those in the 0-24 age category.

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

# Problem 4

The dataset **lungcancer.csv** has information of mortality by age and smoking status.

The dataset contains the following variables:

* Age at baseline: 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80+
* Smoking status: 1 = never smoked, 2 = smoked cigars only, 3 = smoked cigarettes and cigars, 4 = smoked cigarettes only
* Population: number of male pensioners followed
* Deaths: number of deaths in a six-year period

Fit an appropriate model that describes mortality using age at baseline and smoking status.

## Solution for Question 4

Let the response variable, $Y = mortality$, a variable consisting of 2 categories

If the subject is dead, $Y=1$ and if the subject is living, $Y=0$.

Let $X_1 = age$ (age at baseline status), a variable consisting of 9 categories

Let $X_2 = smoking \ status$, a variable consisting of 4 categories

```{r Q4data, echo=FALSE}
lungcancer <- read.csv('lungcancer.csv')

lungcancer$age <- as.character(lungcancer$age) # converting from factor to a charactor vector
living <- lungcancer$population-lungcancer$dead

age.smoke1 <- c(rep(lungcancer$age[1],lungcancer$population[1]),
                rep(lungcancer$age[2],lungcancer$population[2]),
                rep(lungcancer$age[3],lungcancer$population[3]),
                rep(lungcancer$age[4],lungcancer$population[4]),
                rep(lungcancer$age[5],lungcancer$population[5]),
                rep(lungcancer$age[6],lungcancer$population[6]),
                rep(lungcancer$age[7],lungcancer$population[7]),
                rep(lungcancer$age[8],lungcancer$population[8]),
                rep(lungcancer$age[9],lungcancer$population[9]))

mortality.smoke1 <- c(rep(0, living[1]), rep(1, lungcancer$dead[1]),
                      rep(0, living[2]), rep(1, lungcancer$dead[2]),
                      rep(0, living[3]), rep(1, lungcancer$dead[3]),
                      rep(0, living[4]), rep(1, lungcancer$dead[4]),
                      rep(0, living[5]), rep(1, lungcancer$dead[5]),
                      rep(0, living[6]), rep(1, lungcancer$dead[6]),
                      rep(0, living[7]), rep(1, lungcancer$dead[7]),
                      rep(0, living[8]), rep(1, lungcancer$dead[8]),
                      rep(0, living[9]), rep(1, lungcancer$dead[9]))

age.smoke2 <- c(rep(lungcancer$age[10],lungcancer$population[10]),
                rep(lungcancer$age[11],lungcancer$population[11]),
                rep(lungcancer$age[12],lungcancer$population[12]),
                rep(lungcancer$age[13],lungcancer$population[13]),
                rep(lungcancer$age[14],lungcancer$population[14]),
                rep(lungcancer$age[15],lungcancer$population[15]),
                rep(lungcancer$age[16],lungcancer$population[16]),
                rep(lungcancer$age[17],lungcancer$population[17]),
                rep(lungcancer$age[18],lungcancer$population[18]))

mortality.smoke2 <- c(rep(0, living[10]), rep(1, lungcancer$dead[10]),
                      rep(0, living[11]), rep(1, lungcancer$dead[11]),
                      rep(0, living[12]), rep(1, lungcancer$dead[12]),
                      rep(0, living[13]), rep(1, lungcancer$dead[13]),
                      rep(0, living[14]), rep(1, lungcancer$dead[14]),
                      rep(0, living[15]), rep(1, lungcancer$dead[15]),
                      rep(0, living[16]), rep(1, lungcancer$dead[16]),
                      rep(0, living[17]), rep(1, lungcancer$dead[17]),
                      rep(0, living[18]), rep(1, lungcancer$dead[18]))

age.smoke3 <- c(rep(lungcancer$age[19],lungcancer$population[19]),
                rep(lungcancer$age[20],lungcancer$population[20]),
                rep(lungcancer$age[21],lungcancer$population[21]),
                rep(lungcancer$age[22],lungcancer$population[22]),
                rep(lungcancer$age[23],lungcancer$population[23]),
                rep(lungcancer$age[24],lungcancer$population[24]),
                rep(lungcancer$age[25],lungcancer$population[25]),
                rep(lungcancer$age[26],lungcancer$population[26]),
                rep(lungcancer$age[27],lungcancer$population[27]))

mortality.smoke3 <- c(rep(0, living[19]), rep(1, lungcancer$dead[19]),
                      rep(0, living[20]), rep(1, lungcancer$dead[20]),
                      rep(0, living[21]), rep(1, lungcancer$dead[21]),
                      rep(0, living[22]), rep(1, lungcancer$dead[22]),
                      rep(0, living[23]), rep(1, lungcancer$dead[23]),
                      rep(0, living[24]), rep(1, lungcancer$dead[24]),
                      rep(0, living[25]), rep(1, lungcancer$dead[25]),
                      rep(0, living[26]), rep(1, lungcancer$dead[26]),
                      rep(0, living[27]), rep(1, lungcancer$dead[27]))

age.smoke4 <- c(rep(lungcancer$age[28],lungcancer$population[28]),
                rep(lungcancer$age[29],lungcancer$population[29]),
                rep(lungcancer$age[30],lungcancer$population[30]),
                rep(lungcancer$age[31],lungcancer$population[31]),
                rep(lungcancer$age[32],lungcancer$population[32]),
                rep(lungcancer$age[33],lungcancer$population[33]),
                rep(lungcancer$age[34],lungcancer$population[34]),
                rep(lungcancer$age[35],lungcancer$population[35]),
                rep(lungcancer$age[36],lungcancer$population[36]))

mortality.smoke4 <- c(rep(0, living[28]), rep(1, lungcancer$dead[28]),
                      rep(0, living[29]), rep(1, lungcancer$dead[29]),
                      rep(0, living[30]), rep(1, lungcancer$dead[30]),
                      rep(0, living[31]), rep(1, lungcancer$dead[31]),
                      rep(0, living[32]), rep(1, lungcancer$dead[32]),
                      rep(0, living[33]), rep(1, lungcancer$dead[33]),
                      rep(0, living[34]), rep(1, lungcancer$dead[34]),
                      rep(0, living[35]), rep(1, lungcancer$dead[35]),
                      rep(0, living[36]), rep(1, lungcancer$dead[36]))

mortality <- as.factor(c(mortality.smoke1, mortality.smoke2,
                         mortality.smoke3, mortality.smoke4))

age <- as.factor(c(age.smoke1, age.smoke2,
                   age.smoke3, age.smoke4))

smoking.status <- as.factor(c(rep(1, length(age.smoke1)),rep(2,length(age.smoke2)),
                              rep(3, length(age.smoke3)),rep(4,length(age.smoke4))))
```

Based on the available data, a binary logistic regression model can be used because the response variable has only two categories.

A binary logistic regression model can either be conditional or unconditional.

An unconditional regression model is expressed as

$$ln \bigg[ \frac{P(Y=1)}{P(Y=0)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$

$$ln \bigg[ \frac{P(Y=1)}{P(Y=0)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$

$$\frac{P(Y=1)}{P(Y=0)} = {\Large e}^{\beta_0 + \beta_1 X_1 + \beta_2 X_2}$$

A conditional model can be expressed as:

$$ln \bigg[ \frac{P(Y=1)}{P(Y=0)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3X_1X_2$$

$$ln \bigg[ \frac{P(Y=1)}{P(Y=0)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3X_1X_2$$

We first establish if there is a significant interaction, so we run stepwise regression using the binary conditional regression model as follows:

```{r Q4conditional, echo=FALSE, warning=FALSE, message=FALSE}
Question4Model <- glm(mortality~age+smoking.status+age:smoking.status, family = binomial) # the full model

Question4Model_null <- glm(mortality~1, family = binomial) # the null model

summary(Question4Model)

step(Question4Model_null, direction = 'both', scope = formula(Question4Model))
```

Based on the above results, the interaction model had a slightly higher AIC than the best model, which turned out to be an unconditional binary regression model.

Furthermore, out of the 24 interaction dummy variables, only 2 were statistically significant because their p-values were < 0.05.

A more detailed description of the best model findings were as follows:

```{r Q4bestmodel, echo=FALSE, message=FALSE, warning=FALSE}
Q4BestModel <- glm(formula = mortality ~ age + smoking.status, family = binomial); summary(Q4BestModel)
```

$~~$

### Statistical significance of the binary logistic regression model

The findings of the Likelihood Ratio Test were as follows: 

```{r Q4lrtest, echo=FALSE}
Question4Model_null <- glm(mortality~1, family = binomial) # the null model
anova(Question4Model_null, Q4BestModel, test="LRT")
```

Based on the results, this model is statistically significant because the p-values of the Likelihood Ratio Test Statistic is << 0.05.

$~~$

### Statistical significance of coefficients

All coefficients associated with dummy variables derived from $X_1$ (with 40-44 age category as the reference), were statistically significant, with p-values << 0.05.

However, with the smoking status of 1 as the reference, only 2 of the 3 dummy variables derived from $X_2$ were statistically significant, with p-values << 0.05. the dummy variable for smoking status 2 (people who smoked cigars) was not statistically significant because its p-value was > 0.05.

$~~$

### Interpretation of parameter estimates

The adjusted odds ratio for the intercept and coefficients with a 95% confidence interval were as follows:

```{r Q4oratios, echo=FALSE, message=FALSE}
Q4O.R.results <- cbind(exp(coef(Q4BestModel)), exp(confint(Q4BestModel)))

Q4O.R.results <- round(Q4O.R.results, 4)

Q4O.R.results <- as.data.frame(Q4O.R.results)

colnames(Q4O.R.results) <- c("O.R.", "2.5 %", "97.5 %"); Q4O.R.results
```

1. **Age**
   
    The findings for the different categories were as follows:
   
    i)    Adjusting for smoking status, people whose ages fall in the 45-49 category were 77.61% more likely to die compared to those in the 40-44 age category.
   
    ii)   Adjusting for smoking status, people whose ages fall in the 50-54 category were 3 times more likely to die compared to those in the 40-44 age category.
   
    iii)  Adjusting for smoking status, people whose ages fall in the 55-59 category were 4 times more likely to die compared to those in the 40-44 age category.
   
    iv)   Adjusting for smoking status, people whose ages fall in the 60-64 category were 6 times more likely to die compared to those in the 40-44 age category.
   
    v)    Adjusting for smoking status, people whose ages fall in the 65-69 category were 9 times more likely to die compared to those in the 40-44 age category.
   
    vi)   Adjusting for smoking status, people whose ages fall in the 70-74 category were 13 times more likely to die compared to those in the 40-44 age category.
   
    vii)  Adjusting for smoking status, people whose ages fall in the 75-79 category were 21 times more likely to die compared to those in the 40-44 age category.
   
   viii)  Adjusting for smoking status, people whose ages fall in the 80+ category were 34 times more likely to die compared to those in the 40-44 age category.
   
   $~~$

2.  **Smoking status**

    The findings for the different categories were as follows:

    i)    Adjusting for age, people who smoked cigars only were 8.38% more likely to die compared to those who have never smoked.  

    ii)   Adjusting for age, people who smoked cigarettes and cigars were 33.26% more likely to die compared to those who have never smoked. 

    iii)  Adjusting for age, people who smoked cigarettes only were 71.81% more likely to die compared to those who have never smoked. 

$$~~$$

# Problem 5

The data **mental.csv** comes from a study of mental health for a random sample of adult residents of Alachua County, Florida.

Mental impairment is ordinal, with categories (well, mild symptom formation, moderate symptom formation, impaired).

The study related mental impairment(mental) to two predictor variables;

* socioeconomic status (ses)(1=high, 0=low) and
* life events index (event) which is a composite measure of the number and severity of important life events such as birth of a child, a new job, divorce, or a death in family that occurred to the subject within the past three years.

(a) Fit a multinomial logistic model and interpret the results.
(b) Fit an ordinal logistic regression model and interpret the results.
(c) Predict the mental impairment of an individual in high socio economic status with a index of 8 using both model fits. Compare the results.

$~~$

## Solution for Question 5

### (a) Multinomial logistic model

A multinomial logistic model is simply one where the response variable has *m* categories where $m>2$, and one reference variable is used to build $m-1$ number of binary logistic models within this one model.

```{r Q5data, echo=FALSE}
mental <- read.csv('mental.csv')
# make mental an ordered factor
mental.impairment <- factor(mental$mental)

mental.impairment <- ordered(mental.impairment,
                             levels = c(0, 1, 2, 3))

socioeconomic.status <- factor(mental$ses)

life.events.index <- mental$event
```

Let

*well*=0 $<$ *mild symptom formation*=1 $<$ *moderate symptom formation*=2 $<$ *impaired*=3

Let

$X_1 = SES$ (Socio-economic status) and $X_2 = events$ (the life events index)

$$ln \bigg[ \frac{P(Y=1)}{P(Y=0)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$

$$ln \bigg[ \frac{P(Y=2)}{P(Y=0)} \bigg] = \beta_3 + \beta_4 X_1 + \beta_5 X_2$$

$$ln \bigg[ \frac{P(Y=3)}{P(Y=0)} \bigg] = \beta_6 + \beta_7 X_1 + \beta_8 X_2$$

When SES is high, $X_1=1$ and when it is low, $X_1=0$

$~~$

Stepwise regression using the multinomial logistic model was performed and the findings were as follows:

```{r Q5multinom, echo=FALSE}
library(nnet)

Question5Model1 <- multinom(mental.impairment ~ socioeconomic.status + life.events.index) # the full model

Question5Model1_null <- multinom(mental.impairment~1) # the null model

step(Question5Model1_null, direction = 'both', scope = formula(Question5Model1))
```

$~~$

A more detailed description of the best model findings were as follows:

```{r Q5bestmodel1, echo=FALSE, message=FALSE, warning=FALSE}
Q5BestModel1 <- multinom(formula = mental.impairment ~ life.events.index, family = binomial); summary(Q5BestModel1)
```

$~~$

#### Statistical significance of the model:

$~~$

A likelihood ratio test was conducted and the results were as follows:

```{r Q5lrtest1, echo=FALSE}
anova(Question5Model1_null, Q5BestModel1, test="Chisq")
```

The p-value is > 0.05 $\therefore$ the model is not statistically significant.

$~~$

#### Statistical significance of the coefficient

##### Comparing mild mental illness to mental wellness: *Life events*

$~~$

$H_0: \beta_{events} = 0$ $~~$ vs. $~~$ $H_1: \beta_{events} \neq 0$

*Let $\alpha = 0.05$ be the level of significance*

*Let z be the test statistic*
    
$$z_{cal} = \frac{\beta_{events}}{s.e. \beta_{events}} = \frac{0.2174}{0.1781} = 1.2207$$

$$z_{tab} = z_{0.975} = 1.96$$

$z_{cal} < z_{tab}$ $\therefore$ fail to reject $H_0$; the statistical association between life events and the dependent variable is not significant.

$~~$

##### Comparing moderate mental illness to mental wellness: *Life events*

$~~$

$H_0: \beta_{events} = 0$ $~~$ vs. $~~$ $H_1: \beta_{events} \neq 0$

*Let $\alpha = 0.05$ be the level of significance*

*Let z be the test statistic*
    
$$z_{cal} = \frac{\beta_{events}}{s.e. \beta_{events}} = \frac{0.2012}{0.2020} = 0.9960$$
    
$$z_{tab} = z_{0.975} = 1.96$$
    
$z_{cal} < z_{tab}$ $\therefore$ fail to reject $H_0$; the statistical association between life events and the dependent variable holding all other variables constant is not significant.

$~~$

##### Comparing mental impairment to mental wellness: *Life events*

$~~$

$H_0: \beta_{events} = 0$ $~~$ vs. $~~$ $H_1: \beta_{events} \neq 0$

*Let $\alpha = 0.05$ be the level of significance*

*Let z be the test statistic*
    
$$z_{cal} = \frac{\beta_{events}}{s.e. \beta_{events}} = \frac{0.4847}{0.2015} = 2.4055$$
    
$$z_{tab} = z_{0.975} = 1.96$$
    
$z_{cal} > z_{tab}$ $\therefore$ reject $H_0$; the statistical association between life events and the dependent variable is significant.

$~~$

#### Interpretation of parameter estimates:

##### Comparing mild mental illness to mental wellness: *Life events*

$$O.R._{events} = {\Large e}^{0.2174} = 1.2428$$

For every unit increase in life events a person is 24.28% more likely to have mild mental illness.

$~~$

##### Comparing moderate mental illness to mental wellness: *Life events*

$$O.R._{events} = {\Large e}^{0.2012} = 1.2229$$

For every unit increase in life events a person is 22.29% more likely to have moderate mental illness.

$~~$

##### Comparing mental impairment to mental wellness: *Life events*

$$O.R._{events} = {\Large e}^{0.4847} = 1.6237$$

For every unit increase in life events, holding socioeconomic status constant, a person is 62.37% more likely to have moderate mental illness.

$$~~$$

### (b) Ordinal logistic model

An ordinal logistic model is where the event under study is a category or set of lower-ranked categories

Let

*well*=0 $<$ *mild symptom formation*=1 $<$ *moderate symptom formation*=2 $<$ *impaired*=3

$$\frac{P(Y\leq0)}{P(Y>0)} = \frac{P(Y=0)}{P(Y=1) \ or \ P(Y=2) \ or \ P(Y=3)}$$

$$\frac{P(Y\leq1)}{P(Y>1)} = \frac{P(Y=0) \ or \ P(Y=1)}{P(Y=2) \ or \ P(Y=3)}$$

$$\frac{P(Y\leq2)}{P(Y>2)} = \frac{P(Y=0) \ or \ P(Y=1) \ or \ P(Y=2)}{(Y=3)}$$


Let

$X_1 = SES$ (Socio-economic status) and $X_2 = events$ (the life events index)

$$ln \bigg[ \frac{P(Y \leq 0)}{P(Y>0)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$

$$ln \bigg[ \frac{P(Y \leq 1)}{P(Y>1)} \bigg] = \beta_3 + \beta_1 X_1 + \beta_2 X_2$$

$$ln \bigg[ \frac{P(Y \leq 2)}{P(Y>2)} \bigg] = \beta_4 + \beta_1 X_1 + \beta_2 X_2$$

When SES is high, $X_1=1$ and when it is low, $X_1=0$

$~~$

Stepwise regression using the ordinal logistic model was performed and the findings were as follows:

```{r Q5lor, echo=FALSE}
library(MASS)

Question5Model2 <- polr(mental.impairment ~ socioeconomic.status + life.events.index)

Question5Model2_null <- polr(mental.impairment~1) # the null model

step(Question5Model2_null, direction = 'both', scope = formula(Question5Model2))
```

$~~$

Therefore, the best model was as follows:

```{r Q5bestmodel2, echo=FALSE, message=FALSE, warning=FALSE}
Q5BestModel2 <- polr(formula = mental.impairment ~ life.events.index + socioeconomic.status); summary(Q5BestModel2)
```

$~~$

#### Statistical significance of the model:

$~~$

A likelihood ratio test was conducted and the results were as follows:

```{r Q5lrtest2, echo=FALSE, message=FALSE, warning=FALSE}
anova(Question5Model2_null, Q5BestModel2, test="Chisq")
```

The p-value is < 0.05 $\therefore$ the model is statistically significant.

$~~$

#### Statistical significance of coefficients:

1.  **Life events**

    $H_0: \beta_{events} = 0$ vs. $H_1: \beta_{events} \neq 0$

    *Let $\alpha = 0.05$ be the level of significance*

    *Let z be the test statistic*
    
    $$z_{cal} = \frac{\beta_{events}}{s.e. \beta_{events}} = \frac{0.3189}{0.1210} = 2.6355$$
    
    $$z_{tab} = z_{0.975} = 1.96$$
    
    $z_{cal} > z_{tab}$ $\therefore$ reject $H_0$; the statistical association between life events and the dependent variable variable holding all other variables constant is significant.

    $~~$

2.  **SES**

    $H_0: \beta_{SES} = 0$ vs. $H_1: \beta_{SES} \neq 0$

    *Let $\alpha = 0.05$ be the level of significance*

    *Let z be the test statistic*
    
    $$z_{cal} = \frac{\beta_{SES}}{s.e. \beta_{SES}} = \frac{-1.1112}{0.6109} = -1.8190$$
    
    $$z_{tab} = z_{0.025} = -1.96$$
    
    $z_{cal} > z_{tab}$ $\therefore$ fail to reject $H_0$; the statistical association between SES and the dependent variable variable holding all other variables constant is not significant.
    
$~~$

#### Interpretation of parameter estimates:

1.  **Life events**

    $$O.R._{events} = {\Large e}^{0.3189} = 1.3756$$

    For every unit increase in life events, holding socioeconomic status constant, a person is 37.56% more likely to have mild mental illness symptoms or to have moderate mental illness symptoms or to be mentally impaired (with having mental wellness as the reference category).
    
    $~~$

2.  **SES**

    $$O.R._{SES} = {\Large e}^{ -1.1112} = 0.3292$$

    A person with a high socioeconomic status, holding life events constant, is 67.08% less likely to have mild mental illness symptoms or to have moderate mental illness symptoms or to be mentally impaired (with having mental wellness as the reference category) compared to people of low socioeconomic status.
    
$$~~$$

### (c) Predicting the mental impairment of an individual

#### *Prediction of mental impairment using the multinomial model*

$$\frac{P(Y=1)}{P(Y=0)} = {\Large e}^{\beta_0 + \beta_1 X_2} = {\Large e}^{-0.7802 -0.2174X_2}$$

$$\frac{P(Y=2)}{P(Y=0)} = {\Large e}^{\beta_2 + \beta_3 X_2} = {\Large e}^{-1.2513 + 0.2012X_2}$$

$$\frac{P(Y=3)}{P(Y=0)} = {\Large e}^{\beta_4 + \beta_5 X_2} = {\Large e}^{-2.4536+0.4847X_2}$$

$~~$

If $X_1=1$ and $X_2=8$,

$$\frac{P(Y=1)}{P(Y=0)} = {\Large e}^{-0.7802 -0.2174(8)} = {\Large e}^{-2.5194} = 0.0805$$

$$\frac{P(Y=2)}{P(Y=0)} = {\Large e}^{-1.2513 + 0.2012(8)} = {\Large e}^{0.3583} = 1.4309$$

$$\frac{P(Y=3)}{P(Y=0)} = {\Large e}^{-2.4536+0.4847(8)} = {\Large e}^{1.4240} = 4.1537$$

$~~$

i) $$\frac{P(Y=1)}{P(Y=0)} = 0.0805$$
    
   $$P(Y=0) \times \frac{P(Y=1)}{P(Y=0)} = 0.0805[P(Y=0)]$$
    
   $$P(Y=1) = 0.0805[P(Y=0)]$$
    
   $~~$

ii) $$\frac{P(Y=2)}{P(Y=0)} = 1.4309$$

    $$P(Y=0) \times \frac{P(Y=2)}{P(Y=0)} = 1.4309[P(Y=0)]$$

    $$P(Y=2) = 1.4309[P(Y=0)]$$

    $~~$

iii) $$\frac{P(Y=3)}{P(Y=0)} =  4.1537$$

     $$P(Y=0) \times \frac{P(Y=3)}{P(Y=0)} =  4.1537[P(Y=0)]$$

     $$P(Y=3) =  4.1537[P(Y=0)]$$

iv) $$P(Y=0) + P(Y=1) + P(Y=2) + P(Y=3) = 1$$

    $$P(Y=0) +  0.0805[P(Y = 0)] + 1.4309[P(Y = 0)] + 4.1537[P(Y = 0)] = 1$$
    
    $$[1+0.0805+1.4309+4.1537]P(Y = 0) = 1$$
    
    $$\frac{6.6651[P(Y = 0)]}{6.6651} = \frac{1}{6.6651}$$
    
    $$P(Y = 0) = 0.1500$$

$$\therefore$$

$$P(Y = 1) = 0.0805[P(Y = 0)] = 0.0805 \times 0.1500 = 0.0121$$

$$P(Y = 2) = 1.4309[P(Y = 0)] = 1.4309 \times 0.1500 = 0.2146$$

$$P(Y = 3) = 4.1537[P(Y = 0)] = 4.1537 \times 0.1500 = 0.6231$$

$~~$

Based on the results, an individual with a life events index of 8 is most likely to become mentally impaired; this outcome had the highest probability of 0.6231.

$~~$

#### *Prediction of mental impairment using the ordinal logistic model*

$$\frac{P(Y \leq 0)}{P(Y>0)} = {\Large e}^{\beta_0 + \beta_1 X_1 + \beta_2 X_2} = {\Large e}^{-0.2819 -1.1112X_1 + 0.3189X_2}$$

$$\frac{P(Y \leq 1)}{P(Y>1)} = {\Large e}^{\beta_3 + \beta_1 X_1 + \beta_2 X_2} = {\Large e}^{1.2128 -1.1112X_1 + 0.3189X_2}$$

$$\frac{P(Y \leq 2)}{P(Y>2)} = {\Large e}^{\beta_4 + \beta_1 X_1 + \beta_2 X_2} = {\Large e}^{2.2094 -1.1112X_1 + 0.3189X_2}$$

$~~$

If $X_1=1$ and $X_2=8$,

$$\frac{P(Y \leq 0)}{P(Y>0)} = {\Large e}^{-0.2819 -1.1112(1) + 0.3189(8)} = {\Large e}^{1.1581} = 3.1839$$

$$\frac{P(Y \leq 1)}{P(Y>1)} = {\Large e}^{1.2128 -1.1112(1) + 0.3189(8)} = {\Large e}^{2.6528} = 14.1937$$

$$\frac{P(Y \leq 2)}{P(Y>2)} = {\Large e}^{ 2.2094 -1.1112(1) + 0.3189(8)} = {\Large e}^{3.6494} = 38.4516$$

$~~$

i) $$P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3) = 1$$
   
   $$P(Y=0) = 1 - P(Y=1) - P(Y=2) - P(Y=3)$$
   
   $~~$

ii) $$\frac{P(Y \leq 0)}{P(Y>0)} = 3.1839$$

    $$\frac{P(Y=0)}{P(Y=1)+P(Y=2)+P(Y=3)} = 3.1839$$
    
    $$[P(Y=1)+P(Y=2)+P(Y=3)] \times \frac{P(Y=0)}{P(Y=1)+P(Y=2)+P(Y=3)} = 3.1839[P(Y=1)+P(Y=2)+P(Y=3)]$$
    
    $$P(Y=0) = 3.1839[P(Y=1)+P(Y=2)+P(Y=3)]$$
    
    $$1 - P(Y=1) - P(Y=2) - P(Y=3) = 3.1839[P(Y=1)]+3.1839[P(Y=2)]+3.1839[P(Y=3)]$$
    
    $$1 = [3.1839+1]P(Y=1)+[3.1839+1]P(Y=2)+[3.1839+1]P(Y=3)$$
    
    $$\frac{1}{4.1839} = \frac{4.1839[P(Y=1)+P(Y=2)+P(Y=3)]}{4.1839}$$
    
    $$P(Y=1)+P(Y=2)+P(Y=3) = 0.2390$$
    
    $$P(Y=1) = 0.2390-P(Y=2)-P(Y=3)$$
    
    $~~$

iii) $$\frac{P(Y \leq 1)}{P(Y>1)} = 14.1937$$

     $$\frac{P(Y=0)+P(Y=1)}{P(Y=2)+P(Y=3)} = 14.1937$$
     
     $$\frac{[1-P(Y=1)-P(Y=2)-P(Y=3)]+P(Y=1)}{P(Y=2)+P(Y=3)} = 14.1937$$
     
     $$\frac{1+[1-1]P(Y=1)-P(Y=2)-P(Y=3)}{P(Y=2)+P(Y=3)} = 14.1937$$
     
     $$\frac{1-P(Y=2)-P(Y=3)}{P(Y=2)+P(Y=3)} = 14.1937$$
     
     $$[P(Y=2)+P(Y=3)] \times \frac{1-P(Y=2)-P(Y=3)}{P(Y=2)+P(Y=3)} = 14.1937[P(Y=2)+P(Y=3)]$$
     
     $$1-P(Y=2)-P(Y=3) = 14.1937[P(Y=2)]+14.1937[P(Y=3)]$$
     
     $$1 = [14.1937+1]P(Y=2)+[14.1937+1]P(Y=3)$$
     
     $$\frac{1}{15.1937} = \frac{15.1937[P(Y=2)+P(Y=3)]}{15.1937}$$
     
     $$P(Y=2)+P(Y=3) = 0.0658$$
     
     $$P(Y=2) = 0.0658 + P(Y=3)$$
     
     $~~$

iv) $$\frac{P(Y \leq 2)}{P(Y>2)} = 38.4516$$

    $$\frac{P(Y=0)+P(Y=1)+P(Y=2)}{P(Y=3)} = 38.4516$$
    
    $$\frac{[1-P(Y=1)-P(Y=2)-P(Y=3)]+P(Y=1)+P(Y=2)}{P(Y=3)} = 38.4516$$
    
    $$\frac{1+[1-1]P(Y=1)+[1-1]P(Y=2)-P(Y=3)}{P(Y=3)} = 38.4516$$
    
    $$\frac{1-P(Y=3)}{P(Y=3)} = 38.4516$$
    
    $$P(Y=3) \times \frac{1-P(Y=3)}{P(Y=3)} = 38.4516[P(Y=3)]$$
    
    $$1-P(Y=3) = 38.4516[P(Y=3)]$$
    
    $$1 = [38.4516+1]P(Y=3)$$
    
    $$\frac{1}{39.4516} = \frac{39.4516[P(Y=3)]}{39.4516}$$
    
    $$P(Y=3) =  0.0253$$

$$\therefore$$

$$P(Y = 2) =  0.0658 + P(Y = 3) =  0.0658 + 0.0253 = 0.0911$$

$$P(Y=1) = 0.2390 - P(Y=2) - P(Y=3) = 0.2390 - 0.0911 - 0.0253 = 0.1226$$

$$P(Y = 0) = 1 - P(Y=1) - P(Y=2) - P(Y=3) = 1 - 0.1226 - 0.0911 - 0.0253 = 0.7610$$

$~~$

Based on the results, an individual of a high socio-economic status with a life events index of 8 is most likely to have mental wellness; this outcome had the highest probability of 0.7610.

$~~$

#### *Comparison of both predictions*

$~~$

The predictions from both models were on complete ends of the spectrum; the multinomial model predicted that the individual under study would be mentally impaired while the ordinal logistic model prediction was that the individual was mentally well. It would therefore indicate that choice of model to be used for analysis is critical.

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

# Problem 6

A clinical trial for the treatment of small-cell lung cancer was carried out where patients were randomly assigned to two treatment groups:

* Sequential therapy (the same combination of chemotherapeutic agents administered in each treatment cycle)
* Alternating therapy (three different combinations alternated from cycle to cycle)

Gender was considered a potential effect modifier.

The results of the trial were as follows:

Response to therapy was categorized as either

* Progressive disease,
* No change,
* Partial remission or
* Complete remission

Treatment therapy | Gender $~~$ $~~$ $~~$ | Progressive Disease $~~$ | No $~~$ Change  | Partial Remission | Complete Remission
-------------- | ---------- | ------------ | --------- | ------------- | --------------
Sequential        | Male   | 68 | 51 | 39 | 36
$~~$            | Female | 14 | 27 | 12 | 9
$~~$ | $~~$ | $~~$ | $~~$ | $~~$ | $~~$
Alternating       | Male   | 83 | 87 | 56 | 60
$~~$            | Female | 32 | 17 | 12 | 7

Fit a proportional odds logit model and interpret the estimated treatment effect.

$~~$

## Solution for Question 6

A proportional odds logit model is one where the intercept is dependent on the category level.

When we observe the categories of the dependent variable (Response to therapy), *Progressive disease* should be the lowest rank (1) because it indicates that the drug under study is worsening the patient's disease and *Complete remission* should be the highest rank (4) because it indicates that the drug completely cured the patient - it is the result that we would like to see most.

Let

*Progressive disease*=1 $<$ *No change*=2 $<$ *Partial remission*=3 $<$ *Complete remission*=4

$$\frac{P(Y\leq1)}{P(Y>1)} = \frac{P(Y=1)}{P(Y=2) \ or \ P(Y=3) \ or \ P(Y=4)}$$

$$\frac{P(Y\leq2)}{P(Y>2)} = \frac{P(Y=1) \ or \ P(Y=2)}{P(Y=3) \ or \ P(Y=4)}$$

$$\frac{P(Y\leq3)}{P(Y>3)} = \frac{P(Y=1) \ or \ P(Y=2) \ or \ P(Y=3)}{(Y=4)}$$

Let

$X_1 = Therapy$ and $X_2 = Gender$

$$ln \bigg[ \frac{P(Y \leq 1)}{P(Y>1)} \bigg] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$

$$ln \bigg[ \frac{P(Y \leq 2)}{P(Y>2)} \bigg] = \beta_3 + \beta_1 X_1 + \beta_2 X_2$$

$$ln \bigg[ \frac{P(Y \leq 3)}{P(Y>3)} \bigg] = \beta_4 + \beta_1 X_1 + \beta_2 X_2$$

When treatment is sequential, $X_1=1$ and when it is alternating, $X_1=0$

When the patient is male, $X_2=1$ and when they are female, $X_2=0$

Once this data was used to fit a proportional odds logit model, the findings were as follows:

```{r Q6data, echo=FALSE}
# Load the data into R
Sequential = rep(1,(68+14+51+27+39+12+36+9))
Alternating = rep(0,(83+32+87+17+56+12+60+7))

Therapy = c(Sequential, Alternating)

SequentialMale = rep(1,(68+51+39+36))
SequentialFemale = rep(0,(14+27+12+9))
AlternatingMale = rep(1,(83+87+56+60))
AlternatingFemale = rep(0,(32+17+12+7))

Gender = c(SequentialMale, SequentialFemale,
           AlternatingMale, AlternatingFemale)

SM1 = rep(1,68) # Progressive disease, Sequential, Male
SM2 = rep(2,51) # No change, Sequential, Male
SM3 = rep(3,39) # Partial remission, Sequential, Male
SM4 = rep(4,36) # Complete remission, Sequential, Male

SF1 = rep(1,14) # Progressive disease, Sequential, Female
SF2 = rep(2,27) # No change, Sequential, Female
SF3 = rep(3,12) # Partial remission, Sequential, Female
SF4 = rep(4,9) # Complete remission, Sequential, Female

AM1 = rep(1,83) # Progressive disease, Alternating, Male
AM2 = rep(2,87) # No change, Alternating, Male
AM3 = rep(3,56) # Partial remission, Alternating, Male
AM4 = rep(4,60) # Complete remission, Alternating, Male

AF1 = rep(1,32) # Progressive disease, Alternating, Female
AF2 = rep(2,17) # No change, Alternating, Female
AF3 = rep(3,12) # Partial remission, Alternating, Female
AF4 = rep(4,7) # Complete remission, Alternating, Female

ResponseToTherapy = as.factor(c(SM1, SM2, SM3, SM4,
                                SF1, SF2, SF3, SF4,
                                AM1, AM2, AM3, AM4,
                                AF1, AF2, AF3, AF4))

ResponseToTherapy <- ordered(ResponseToTherapy,
                             levels = c(1, 2, 3, 4))

drug_trial_data = as.data.frame(cbind(ResponseToTherapy,
                                      Therapy,
                                      Gender))

```

```{r Q6lor, echo=FALSE}
library(MASS)

Question6Model = polr(ResponseToTherapy ~ Therapy + Gender) # the full model

Question6Model_null <- polr(ResponseToTherapy ~ 1) # the null model

step(Question6Model_null, direction = 'both', scope = formula(Question6Model))
```

$~~$

Therefore, the best model was as follows:

```{r Q6bestmodel, echo=FALSE, message=FALSE, warning=FALSE}
Q6BestModel <- polr(formula = ResponseToTherapy ~ Gender); summary(Q6BestModel)
```

$~~$

### Statistical significance of the model

A likelihood ratio test was conducted and the results were as follows:

```{r Q6lrtest, echo=FALSE, message=FALSE, warning=FALSE}
anova(Question6Model_null, Q6BestModel, test="Chisq")

```

The p-value is > 0.05 $\therefore$ the model is not statistically significant.

$~~$

### Statistical significance of the coefficient: *Gender*

$H_0: \beta_{gender} = 0$ vs. $H_1: \beta_{gender} \neq 0$

*Let $\alpha = 0.05$ be the level of significance*

*Let z be the test statistic*
    
$$z_{cal} = \frac{\beta_{gender}}{s.e. \beta_{gender}} = \frac{0.3109}{0.1767} = 1.7595$$
    
$$z_{tab} = z_{0.975} = 1.96$$
    
$z_{cal} < z_{tab}$ $\therefore$ fail to reject $H_0$; the statistical association between gender and the dependent variable variable holding all other variables constant is not significant.


$~~$

### Interpretation of parameter estimate: *Gender*

$$O.R._{gender} = {\Large e}^{0.3109} = 1.3647$$

A male patient is 36.47% more likely to have progressive disease or no change in illness or partial remission (with the complete remission as the reference category) compared to a female patient.

$$~~$$

$$~~$$

$$~~$$

# Problem 7

Critique the two journal articles on physical inactivity and adolescent risk behaviour.

## Solution for Question 7

### Physical Activity Article Critique

Toriola (2018) in their article A Multinomial Logistic Regression Analysis of the Association Between Physical Activity and Body Weight Status of University Women in Riyadh, Saudi Arabia sought to study the relationship between physical activity level and body weight among the population of university-going women in the kingdom of Saudi Arabia with a view of addressing growing obesity in Saudi Arabia and the Middle East region in general. While the researcher is solving a real-world problem and uses best practice in research study design, data collection and analysis, model comparisons, deeper interpretation of results and findings that take into account more variables were needed to make a stronger model.


#### Introduction

$~~$

The researcher introduces their article by stating that past research indicated that “habitual physical activity lowers CVD [cardiovascular disease] risk and the outcome has been assumed to be facilitated by healthy body weight”. They then further elaborate that people have fallen short of recommended levels of physical activity and body weight at normal values. Furthermore, more and more Saudi women according to the researcher have been found to develop cardiometabolic diseases which have obesity as a research.

The researcher's research is indeed relevant due to more countries experiencing the obesity of their citizens, particularly children, as a challenge (Onis et. al., 2010). The researcher also highlights research from reputable global bodies such as the World Health Organization to make his case.

While the researcher touches on obesity, they do not take into account nutrition which is a key determinant (Bopkin, 2001; Swinburn et. al., 2004); they focus more on physical exercise. It would be helpful to also consider influences outside an individual (Spence and Lee, 2002) such as cultural norms and how they influence physical activity levels, particularly in the Saudi community. These could be a contributing factor to physical inactivity.


#### Research Methodology

$~~$

The researcher selected a cross-sectional survey design, with physical activity levels and Body Mass Index (BMI) characteristics as their variables of interest in the study and a large sample size (n>30) was taken resulting in 573 observations.

Measures of central tendency were used to evaluate the respondents age, weight, height, BMI and METs score. METs scores, in particular, were stratified into three physical activity level categories before measures of central tendency were taken: Low, where METs < 500; Moderate, where 500 $\leq$ METs $\leq$ 1499; Vigorous, where METs > 500. Finally BMI data was put into four categories: Underweight (11.7%), Normal weight (65.1%), Overweight (18.7%) and Obese (4.5%).

One-way ANOVA was conducted on the physical activity level categories with age, body weight, height and BMI as treatments. This was then followed by a multinomial logistic regression model of BMI and physical activity level as categorical variables with BMI as the predictor variable and physical activity level as the dependent variable.

All statistical tests were two-tailed and the 95% confidence level was selected. 

It is commendable that the researcher observed global best practice in getting the respondent’s consent, taking the height and weight of respondents and measuring their physical activity levels. It is also commendable that a pilot test was conducted to ensure reliable data collection and current best practice for data analysis like use of statistical software like SPSS was observed. 

However, using IPAQ as the entire questionnaire limited the study because it is a global standardized questionnaire that may not adequately account for cultural norms. A longitudinal study design would have been beneficial compared to a cross-sectional study to observe additional variables outside of the individual (Spence and Lee, 2002) and minimize the self-reporting aspect that is in the existing study. The categorical variables used in the researcher’s model could also be interpreted as ordinal in nature; therefore an ordinal logistic regression model could have been used and the two models could have been compared in order to select the better model. 


#### Results & Discussion

$~~$

The researcher stressed that “participation in moderate to vigorous PA [physical activity level] irrespective of an individual’s body weight classification is beneficial to health”. The researcher also appreciates that gender, race and ethnicity also affect BMI and has called for further studies that can generate more generalizable findings and have larger sample sizes.

The researcher could have included the statistical significance of the entire multinomial logistic regression model in order for the reader to have a frame of reference for further interpretations of the model. Furthermore, transforming the reported Odds Ratios into percentages would ease interpretation surrounding the likelihoods of being underweight, overweight or obese. Interpretations of likelihood also need to ensure that the results are reported as being compared to the reference category, which in this study was normal weight.


#### Concluding Remark

$~~$

This article is addressing a real-world problem and can open the door for further research into weight problems facing the Middle East.


#### References

$~~$

Toriola, O., 2018. A multinomial logistic regression analysis of the association between physical activity and body weight status of university women in Riyadh, Saudi Arabia. *Asian Journal of Scientific Research*, 11, pp.145-150.

De Onis, M., Blössner, M. and Borghi, E., 2010. Global prevalence and trends of overweight and obesity among preschool children. *The American journal of clinical nutrition*, 92(5), pp.1257-1264.

Popkin, B.M., 2001. The nutrition transition and obesity in the developing world. *The Journal of Nutrition*, 131(3), pp.871S-873S.

Swinburn, B.A., Caterson, I., Seidell, J.C. and James, W.P.T., 2004. Diet, nutrition and the prevention of excess weight gain and obesity. *Public Health Nutrition*, 7(1a), pp.123-146.

Spence, J.C. and Lee, R.E., 2003. Toward a comprehensive model of physical activity. *Psychology of sport and exercise*, 4(1), pp.7-24.

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

### Adolescent Risk Behaviour Article Critique

Peng and Nichols (2003) in their article Using multinomial logistic models to predict adolescent behavioral risk sought to apply multinomial logistic regression models on existing data on a sample of 432 adolescents that were in junior high (Grade 7-9) in 1988. This research was done in order to help psychologists and educators identify adolescents at the greatest behavioral risk and help them differentiate between categories of behavioural risk among adolescents. This differentiation could then help build a profile that can help personalize the prevention programs that psychologists and educators currently offer.

While this article is solving a real-world problem and did statistical analysis best practice such as conducting cross-validation and having a comparison of estimated probabilities with actual categorizations, the authors could consider enriching the article with more past studies that tie their identified covariates to the dependent variable e.g. briefly citing works where psychologists discussed the theoretical link between behavioural risk in adolescents with gender so that they can strengthen their case for selection of that variable. They could have also explored more than one model. For instance, they could have made the FAMILY variable continuous in one instance and categorical in another and evaluated the two; they could have specified an unconditional multinomial regression model where the dependent variable is not an ordered categorical variable and compared it to a different model where the dependent variable is an ordered categorical variable. 

$~~$

#### Introduction

$~~$

The data source selected for the study was given by one of the authors of a 1993 article, G. M. Ingersoll was gathered from 517 junior high school students in 1988; ; upon realizing that there was missing data, those observations were omitted such that the final sample size became 432.

Each student filled two data collection tools to generate a self report of their risky behaviour: Rosenberg’s Self Esteem Inventory and the Health Behaviour Questionnaire. 

Consequently, the researchers declared their research hypothesis as “*the likelihood that an adolescent is at high, medium, or low behavioral risk is related to his/her gender, intention to drop out of school, family structure, emotional risk, and self esteem.*”

Bearing this in mind, the researchers then proceeded to convert the continuous behavioral risk score in the original data to a categorical variable by applying statistical rationale; the low/medium behavioural risk boundary was selected to be the median of the continuous variable and the medium/high behavioural risk boundary was selected to be the sum of the mean and 1 standard deviation of the original variable.

Measures of central tendency were applied to the rest of the variables in the dataset, stratifying the remaining categorical variables by the newly formed dependent variable - the categorical behavioural risk variable (hereafter known as the RISK variable). Most boys when classified fell in the high and medium categories and most children raised in single-parent homes fell in the medium category. This made the gender of the respondent (hereafter referred to as GENDER) and family structure (hereafter referred to as FAMILY) variables worth including in the model. A significant percentage (12.27%) of the respondents indicated that they would like to drop out of school; therefore, the intention of the child to dropout (hereafter referred to as DROPOUT) was included in the model. Finally, their emotional risk score and the self-esteem score of the child (hereafter referred to as EMOTION and ESTEEM respectively) were included in the model.

The multinomial logistic regression model was selected by the researchers because it effectively and reliably demonstrated how the self-reported characteristics of adolescents can impact their behavioural risk level and quantifies their net effect on each dependent variable category relative to the reference category.

The researchers clearly explained the context around data collection which increased the validity of their real-world data set. However, to increase the credibility of their selection criteria for the ESTEEM and EMOTION variables, it would be advisable to include past studies that theoretically link each of them to the dependent variable.

The researchers appear to have selected the ordinal form of the complex multinomial logistic model. It would have been interesting to evaluate a form of this model where the dependent variable was an unordered categorical variable or where FAMILY was a categorical variable instead of a continuous one. Comparing more than one model rather than only using the null model for comparison with the existing model would have allowed further discussion of the different types of complex multinomial models and would have allowed the researchers to select the best model.


$~~$

#### The Multinomial Logistic Model on real-life data

$~~$

Let $Y = 1 \ or \ 2 \ or \ 3$ where low behavioural risk (3) is the reference category; this is an ordered categorical variable

$~~$

$p_1 =$ probability of high behavioural risk $= P(Y=1)$

$p_2 =$ probability of medium behavioural risk $= P(Y=2)$

$~~$

The log of odds of *Y* is also called the Logit of *Y*

$$Logit(p_1) = natural \ log \ (odds) = ln$$

$$Logit(p_1+p_2) = natural \ log \ (odds) = ln$$

$~~$

The model constraint: Spi = 1

$~~$

The constants in the model are first *Y* intercept, $\alpha_1$, the second *Y* intercept, $\alpha_2$ and the gradient of the resulting sigmoid curves, *b*. All three constants are determined by the maximum likelihood method.

$~~$

Let

$X_1 = GENDER$ where $X_1=1$ if a boy and $X_1=0$ if a girl; this is a categorical variable

$X_2 = DROPOUT$ where $X_2=1$ if the response is ‘Yes, I intend to drop out’ and $X_2=0$ if the response is ‘No, I don’t intend to drop out’; this is a categorical variable

$X_3 = FAMILY$ where $X_3=1$ if from an intact family, $X_3=2$ if from a family with 1 step-parent and $X_3=3$ if from a family with a single-parent family; this is a continuous variable

$X_4 = EMOTION$; this is a continuous variable

$X_5 = ESTEEM$; this is a continuous variable

$~~$

Let ${\Large e} = 2.71828$, the base of the system of natural logarithms

$~~$

Regarding the individual covariate coefficients:

* There are two hypotheses to be evaluated

  $H_0: \beta_j = 0$ i.e. each and every one of the coefficients in the model are not statistically significant

  $H_1: \beta_j \neq 0$ i.e. at least one of the coefficients in the model is statistically significant

* The odds ratio ${\Large e}^{\beta_j}$ is the change in the odds of *Y* given a unit change in $X_j$

$~~$

Therefore, the resulting model will be:

$${\Large p}_1 = \frac{{\Large e}^{\alpha_1 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5}}{1 - {\Large e}^{\alpha_1 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5}}$$

$${\Large p}_1 + {\Large p}_2 = \frac{{\Large e}^{\alpha_2 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5}}{1 - {\Large e}^{\alpha_2 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5}}$$

$$p_3 = 1 - (p_1 + p_2)$$

$~~$

#### Model Results

$~~$

The model defined above was fitted onto SAS PROC LOGISTIC (Version 8e, SAS Institute Inc., 1999), statistical computer software, and the resulting model was as follows:

$$Logit(\hat{p}_1) = -0.6211 + 1.1070X_1 + 2.1818X_2 + 0.4135X_3 + 0.00738X_4 - 0.0488X_5$$

$$Logit(\hat{p}_1+\hat{p}_2) = 2.5220 + 1.1070X_1 + 2.1818X_2 + 0.4135X_3 + 0.00738X_4 - 0.0488X_5$$

$~~$

The adjusted O.Rs for each of the covariates were obtained as shown: 


Variable | Variable Name | Coefficient($\beta_j$)  | Adjusted Odds Ratio (${\Large e}^{\beta_j}$)
-------- | ------- | ---------- | ----------
$X_1$    | GENDER  | 1.1070  | 3.0253
$X_2$    | DROPOUT | 2.1818  | 8.8622
$X_3$    | FAMILY  | 0.4135  | 1.5121
$X_4$    | EMOTION | 0.00738 | 1.0074
$X_5$    | ESTEEM  | 0.0488  | 1.0500


Based on the model:

* Boys are 3 times more likely to have high or medium behavioural risk compared to girls, holding all other covariates constant.

* Adolescents that intend to drop out of school are 9 times more likely to have high or medium behavioural risk compared to adolescents that don’t intend to do so, holding all other covariates constant.

* For every unit increase in family structure, adolescents are 51.21% more likely to have high or medium behavioural risk, holding all other covariates constant.

* For every unit increase in their emotional risk score, adolescents are 0.74% more likely to have high or medium behavioural risk, holding all other covariates constant.

* For every unit increase in their self-esteem score, adolescents are 4.76% less likely to have high or medium behavioural risk, holding all other covariates constant.

The researchers therefore concluded that based on the model, a profile for an adolescent with high behavioural risk is a male who intends to drop out of school, comes from a single-parent home, possibly has high emotional risk scores and  low self esteem scores.

The researchers  did well by putting forth a profile; However, they could incorporate a level of significance and a confidence interval for their adjusted O.R.s and interpretations.

$~~$

#### Model Diagnostics

$~~$

The researchers conducted diagnostics in two key ways: performing cross-validation and using a variety of tools to conduct model and coefficient diagnostics. 
They performed cross-validation by splitting the sample randomly into 10 sub-samples. The model was then fit on the overall dataset and each of the sub samples.
For various aspects of model diagnostics, they were careful to use more than one test statistic and explain the limitations of each test where possible.

$~~$

*Significance of the entire model*

The entire model was found to be statistically significant in both the sub-samples and overall sample.

The Likelihood ratio, the Score test and the Wald test all had p-values that were less than 0.0001

Because the Chi-Square Test of Proportional Odds assumption had a p-value of 0.6548, there was no need to fit a second model.

Furthermore, the Hosmer and Lemeshow test was conducted on the individual models that were estimating $p_1$ and $p_1 or p_2$ and both were found statistically significant - the p-values of both test statistics were > 0.4.

$~~$

*Significance of individual covariate coefficients*

All covariate coefficients were found to be statistically significant except EMOTION - this had a p-value that was greater than 0.05 (0.5211).

However, FAMILY and DROPOUT covariate coefficients were found to be statistically significant in 9 out of the 10 sub-samples.

$~~$

*Validation of predictive probabilities*

A number of measures of association were used such as Kendall’s tau-a, Goodman-Kruskal’s Gamma, Sommers’ D statistic and the c statistic. Goodman-Kruskal’s Gamma was 0.548, indicating that fewer errors were made in predicting the probabilities using the model than by using random chance. The c statistic value was 0.769, indicating that the model correctly categorized 76.9% of the possible pairs of adolescents; therefore, the model is better than random chance.

$~~$

*Handling of missing cases*

The researchers discovered that EMOTION was the variable that had most of the missing cases and posited that this may have contributed to the insignificance of its individual coefficient.
Missing data was imputed using SPSS using the EM method, a statistical software, and the model was fitted on the modified data. However, the statistical significance of individual coefficients was the same as the model that was fir on data that had the observations with missing data completely omitted.

The researchers conducted thorough diagnostics, going so far as to select 36 cases, comparing their estimated probabilities to the actual category of the dependent variable that each observation was classified in. This was good because it enabled them to further defend the model and evaluate individual covariate coefficient results.

$~~$

#### Reference

$~~$

Peng, C.Y.J. and Nichols, R.N., 2003. Using multinomial logistic models to predict adolescent behavioral risk. *Journal of Modern Applied Statistical Methods*, 2(1), p.16.

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

$$~~$$

# Problem 8

In at most four pages, briefly discuss generalized estimating equations (GEE) and generalized linear mixed models (GLMM) using an example in each case.

$~~$

## Solution for Question 8

### Generalized Estimating Equations (GEE)

The same subject in a study may change over time; therefore, there may be correlation between the successive measurements and this makes the assumption that the observations in *Y* are independent become implausable.

Furthermore, a subject may have a more similar measurement to another subject within the same group compared to a subject from a different group

The two scenarios above make the observations become correlated with one another. To handle this kind of data, the correlation structure within the data can be modelled. This forms **generalized estimating equations**.

The following example will be used to illustrate how these equations are applied.

#### Example: Per Capita Expenditure

$~~$

*Data source:* https://bit.ly/3blw0be

Per Capita Expenditure (PCE) is the total market value of all purchases in a country divided by that country's total population. It is a measure used to assess the disposable income that the average citizen has in a given country/territory.

Data on per capita expenditure for 120 countries and territories was collected by the World Bank for the period 2000-2018. These countries were grouped into 7 regions:

* East Asia and Pacific
* Europe and Central Asia
* Latin America and the Caribbean
* Middle East and North Africa
* Other High Income
* South Asia
* Sub-Saharan Africa

```{r GEEdata, echo=FALSE}
PCE <- read.csv('PCE data.csv')
per.capita.expenditure <- PCE[,42:59]
per.capita.expenditure$X2018 <- PCE$X2018
per.capita.expenditure$country.or.territory <- PCE$Unnamed..0
per.capita.expenditure$region <- PCE$region
per.capita.expenditure <- per.capita.expenditure[complete.cases(per.capita.expenditure), ]

year.2018 <- as.data.frame(aggregate(per.capita.expenditure$X2018 ~per.capita.expenditure$region, FUN=sum))
year.2017 <- as.data.frame(aggregate(per.capita.expenditure$X2017 ~per.capita.expenditure$region, FUN=sum))
year.2016 <- as.data.frame(aggregate(per.capita.expenditure$X2016 ~per.capita.expenditure$region, FUN=sum))
year.2015 <- as.data.frame(aggregate(per.capita.expenditure$X2015 ~per.capita.expenditure$region, FUN=sum))
year.2014 <- as.data.frame(aggregate(per.capita.expenditure$X2014 ~per.capita.expenditure$region, FUN=sum))
year.2013 <- as.data.frame(aggregate(per.capita.expenditure$X2013 ~per.capita.expenditure$region, FUN=sum))
year.2012 <- as.data.frame(aggregate(per.capita.expenditure$X2012 ~per.capita.expenditure$region, FUN=sum))
year.2011 <- as.data.frame(aggregate(per.capita.expenditure$X2011 ~per.capita.expenditure$region, FUN=sum))
year.2010 <- as.data.frame(aggregate(per.capita.expenditure$X2010 ~per.capita.expenditure$region, FUN=sum))
year.2009 <- as.data.frame(aggregate(per.capita.expenditure$X2009 ~per.capita.expenditure$region, FUN=sum))
year.2008 <- as.data.frame(aggregate(per.capita.expenditure$X2008 ~per.capita.expenditure$region, FUN=sum))
year.2007 <- as.data.frame(aggregate(per.capita.expenditure$X2007 ~per.capita.expenditure$region, FUN=sum))
year.2006 <- as.data.frame(aggregate(per.capita.expenditure$X2006 ~per.capita.expenditure$region, FUN=sum))
year.2005 <- as.data.frame(aggregate(per.capita.expenditure$X2005 ~per.capita.expenditure$region, FUN=sum))
year.2004 <- as.data.frame(aggregate(per.capita.expenditure$X2004 ~per.capita.expenditure$region, FUN=sum))
year.2003 <- as.data.frame(aggregate(per.capita.expenditure$X2003 ~per.capita.expenditure$region, FUN=sum))
year.2002 <- as.data.frame(aggregate(per.capita.expenditure$X2002 ~per.capita.expenditure$region, FUN=sum))
year.2001 <- as.data.frame(aggregate(per.capita.expenditure$X2001 ~per.capita.expenditure$region, FUN=sum))
year.2000 <- as.data.frame(aggregate(per.capita.expenditure$X2000 ~per.capita.expenditure$region, FUN=sum))

agg.PCE <- as.data.frame(cbind(year.2000, year.2001[,2], year.2002[,2],
                               year.2003[,2], year.2004[,2], year.2005[,2],
                               year.2006[,2], year.2007[,2], year.2008[,2],
                               year.2009[,2], year.2010[,2], year.2011[,2],
                               year.2012[,2], year.2013[,2], year.2014[,2],
                               year.2015[,2], year.2016[,2], year.2017[,2],
                               year.2018[,2]))
```

When we take an example of plotting Argentina's and Sub-Saharan Africa's graph below, we see that the individual observations are correlated over time at an observational level and at a group level:

```{r Argentina and Sub-Saharan Africa, echo=FALSE, out.width='50%'}
plot(2000:2018, per.capita.expenditure[48,1:19], title(main = 'Argentina'), xlab = 'Year', ylab = 'Per capita expenditure')
plot(2000:2018, agg.PCE[7,2:20], title(main = 'Sub-Saharan Africa'), xlab = 'Year', ylab = 'Per capita expenditure')
```

If there are $N$ number of subjects(in this case, territories) with $n_i$ measurements for subject $i$,

let $\boldsymbol{y}_i$ denote the vector of responses for each territory and $\boldsymbol{y}$ denote the vector of responses for all territories.

The length of $\boldsymbol{y}$ will be $\sum\limits^N_{i=i} n_i$

In our example, $n_i$ measurements for each of the territories is equal $\therefore$ $n_i = n_j = 19$

$$E(\boldsymbol{y}_i) = \boldsymbol{\mu}_i$$

$$g(\boldsymbol{\mu}_i)=\boldsymbol{X}^T_i \boldsymbol{\beta}$$

$$\boldsymbol{D} = \frac{\partial \boldsymbol{\mu}_i}{\partial \beta_j}$$
Therefore, the **generalized estimating equations** are:

*i.*
$$\boldsymbol{U} = \sum\limits^N_{i=1} \boldsymbol{D}^T_i \boldsymbol{V}^{-1}_{i}(\boldsymbol{y}_i - \boldsymbol{\mu}_i) = 0$$

where

*ii.*
$$\boldsymbol{V}_i = \boldsymbol{A}^{\frac{1}{2}}_i \boldsymbol{R}_i  \boldsymbol{A}^{\frac{1}{2}}_i \phi$$

where

$\boldsymbol{A}$ is a matrix with elements, $var(\boldsymbol{y}_{ik})$
$\boldsymbol{R}_i$ is a correlation matrix for $\boldsymbol{y}_i$ and
$\phi$ is a constant for overdispersion

$~~$

The GEEs are solved iteratively as follows:

1. Start with $\boldsymbol{R}_i = \boldsymbol{I}$ and $\phi = 1$

2. Estimate $\boldsymbol{\beta}$ by solving equations in *i.*

3. Calculate fitted values $\widehat{\boldsymbol{\mu}}_i=g^{-1}(\boldsymbol{X}^T_i) \widehat{\boldsymbol{\beta}}$

4. Get the residuals $\boldsymbol{y}_i-\widehat{\boldsymbol{\mu}}_i$

5. Use the resulting residuals to estimate $\boldsymbol{A}_i$, $\boldsymbol{R}_i$ and $\phi$
   
6. Repeat steps 2, 3, 4 and 5 until convergence is achieved.

$~~$

If the data is continous, correlation is used but when it is binary, odds ratios can be used. Furthermore, for longitudinal data, a sandwich estimator must be used for $var(\boldsymbol{\beta})$ given as

$\boldsymbol{V}_s(\widehat{\boldsymbol{\beta}}) = \Im^{-1}\boldsymbol{C}\Im^{-1}$

where

$\Im = \sum^N_{i=1}\boldsymbol{D}^T_i \widehat{\boldsymbol{V}}^{-1}_i\boldsymbol{D}_i$ and $\boldsymbol{C} = \sum^N_{i=1}\boldsymbol{D}^T_i  \widehat{\boldsymbol{V}}^{-1}_i (\boldsymbol{y}_i-\widehat{\boldsymbol{\mu}}_i)(\boldsymbol{y}_i-\widehat{\boldsymbol{\mu}}_i)^T \boldsymbol{D}_i$

$~~$

Application of this approach on our data (with the Wald test statistic being applied) gives the following statistically significant coefficients:

```{r Q8model1, echo=FALSE, message=FALSE, warning=FALSE}
library(geepack)
library(MESS)

region <- factor(rep(per.capita.expenditure$region,19))
territory <- factor(rep(per.capita.expenditure$country.or.territory,19))
p.c.expenditure <- c(per.capita.expenditure$X2000, per.capita.expenditure$X2001,
                     per.capita.expenditure$X2002, per.capita.expenditure$X2003,
                     per.capita.expenditure$X2004, per.capita.expenditure$X2005,
                     per.capita.expenditure$X2006, per.capita.expenditure$X2007,
                     per.capita.expenditure$X2008, per.capita.expenditure$X2009,
                     per.capita.expenditure$X2010, per.capita.expenditure$X2011,
                     per.capita.expenditure$X2012, per.capita.expenditure$X2013,
                     per.capita.expenditure$X2014, per.capita.expenditure$X2015,
                     per.capita.expenditure$X2016, per.capita.expenditure$X2017,
                     per.capita.expenditure$X2018)
year <- factor(c(rep(2000,120),rep(2001,120),rep(2002,120),rep(2003,120),
                 rep(2004,120),rep(2005,120),rep(2006,120),rep(2007,120),
                 rep(2008,120),rep(2009,120),rep(2010,120),rep(2011,120),
                 rep(2012,120),rep(2013,120),rep(2014,120),rep(2015,120),
                 rep(2016,120),rep(2017,120),rep(2018,120)))

Question8Model1 <- geeglm(p.c.expenditure~region+year+region*year, family=gaussian, id=territory, wave=year, corst="ar1")

library(broom); Q8tidied1 <- tidy(Question8Model1)
library(dplyr); filter(Q8tidied1, Q8tidied1$p.value <0.05)
```

Upon applying the Wald test to the fitted model, it was found to be statistically significant $\because \ \alpha_{cal} << 0.05$ as shown below:

```{r Q8lrtest1, echo=FALSE}
Question8Model1_null <- geeglm(p.c.expenditure~1,family=gaussian, id=territory, wave=year, corst="ar1") # null model
anova(Question8Model1_null,Question8Model1, test='Wald')
```

### Generalized Linear Mixed Models (GLMM)

An Ordinary Least Squares Regression Model is of the form

$$y_i = \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_kx_{ik}+\varepsilon_i$$
$$E(\boldsymbol{y})=\boldsymbol{\mu} = \boldsymbol{X}\boldsymbol{\beta}$$

When we want to use generalized linear models, we expand the class of the response variable from strictly normal distribution to any distribution within the exponential family of distributions, which includes the normal distribution. This is achieved by applying a transformation to the expected value of $\boldsymbol{y}$, $\boldsymbol{\mu}$ as shown:
$$g(\boldsymbol{\mu})=\boldsymbol{X}\boldsymbol{\beta}$$

Therefore, a generalized linear mixed models is where the expected value of the response variable of a generalized linear model has both fixed and random effects i.e.

$$g(\boldsymbol{\mu})=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{Z}\boldsymbol{u}+\boldsymbol{e}$$

where $\boldsymbol{\beta}$ are the fixed effects and the random effects of the model ($\boldsymbol{\varepsilon}$) are split into two, random effects between subjects, $\boldsymbol{Z}\boldsymbol{u}$ and random effects within subjects, $\boldsymbol{e}$.

To make the calculations more easier to handle, particular pairs of distributions are used called  *conjugate distributions*. This approach is therefore similar to Bayesian analysis.

Examples of conjugate distributions include:

* Normal distribution for $\boldsymbol{y}|\boldsymbol{u}$ and Normal distribution for $\boldsymbol{u}$
* Poisson distribution for  $\boldsymbol{y}|\boldsymbol{u}$ and Gamma distribution for $\boldsymbol{u}$
* Binomial distribution for  $\boldsymbol{y}|\boldsymbol{u}$ and Beta distribution for $\boldsymbol{u}$
* Binomial distribution for  $\boldsymbol{y}|\boldsymbol{u}$ and Normal distribution for $\boldsymbol{u}$

#### Example: Alexa Reviews

$~~$

*Data source:* https://go.aws/2WkOfZp

When Amazon customers purchase an Amazon Alexa, a voice-controlled virtual assistant, some of them may choose to leave a review and give the product a rating from one to 5. The data below consists of data surrounding 3,150 customers that left a review. It contains the following variables:

* *rating*: The actual rating that the customer gave a product on a scale of 1 to 5
* *variation*: The outer colour/pattern that the product had
* *date*: The time the customer posted the review
* *love_dummy*: The review contained one of the 10 most frequent verbs, love.
* *great_dummy*: The review contained one of the 10 most frequent verbs, great.
* *like_dummy*: The review contained one of the 10 most frequent verbs, like.
* *easy_dummy*: The review contained one of the 10 most frequent verbs, easy.
* *works_dummy*: The review contained one of the 10 most frequent verbs, works.
* *good_dummy*: The review contained one of the 10 most frequent verbs, good.
* *doesnt_dummy*: The review contained one of the 10 most frequent verbs, doesn't.
* *quality_dummy*: The review contained one of the 10 most frequent verbs, quality.
* *better_dummy*: The review contained one of the 10 most frequent verbs, better.
* *well_dummy*: The review contained one of the 10 most frequent verbs, well.

(Code for all the dummy variables: Yes=1, No=0)

```{r GLMMdata, echo=FALSE}
AlexaReviews <- read.csv('AlexaReviews.csv')

# make the response variable an ordered factor
AlexaReviews$rating <- ordered(AlexaReviews$rating,
                             levels = c(1, 2, 3, 4, 5))

# coerce the date column into actual dates
AlexaReviews$date <- as.Date(AlexaReviews$date, format="%d-%b-%y")

# let 0 be the reference representing other verbs for the dummy variables
AlexaReviews$love_dummy <- factor(AlexaReviews$love_dummy)
AlexaReviews$great_dummy <- factor(AlexaReviews$great_dummy)
AlexaReviews$like_dummy <- factor(AlexaReviews$like_dummy)
AlexaReviews$easy_dummy <- factor(AlexaReviews$easy_dummy)
AlexaReviews$works_dummy <- factor(AlexaReviews$works_dummy)
AlexaReviews$good_dummy <- factor(AlexaReviews$good_dummy)
AlexaReviews$doesnt_dummy <- factor(AlexaReviews$doesnt_dummy)
AlexaReviews$quality_dummy <- factor(AlexaReviews$quality_dummy)
AlexaReviews$better_dummy <- factor(AlexaReviews$better_dummy)
AlexaReviews$well_dummy <- factor(AlexaReviews$well_dummy)
AlexaReviews$customerID <- 1:length(AlexaReviews[,1])
AlexaReviews$variation <- factor(AlexaReviews$variation)
```

A generalized linear mixed model was fitted onto the data by maximum likelihood (Laplace Approximation) with *date* and *variation* as the random effects. This resulted in the following statistically significant fixed-effect coefficients (based on the z test statistic):

```{r Q8model2, echo=FALSE, warning=FALSE, message=FALSE}
library(lme4)
Question8Model2 <- glmer(rating ~ (1|variation) + (1|date) + love_dummy + great_dummy + like_dummy + easy_dummy + works_dummy + good_dummy + doesnt_dummy + quality_dummy + better_dummy + well_dummy + doesnt_dummy:love_dummy + doesnt_dummy:like_dummy,data=AlexaReviews, family=binomial)

Question8Model2_null <- glmer(rating ~ (1|variation) + (1|date) + 1,data=AlexaReviews, family=binomial)


library(broom.mixed); Q8tidied2 <- tidy(Question8Model2)
library(dplyr); filter(Q8tidied2, Q8tidied2$p.value < 0.05)
```

Upon applying the Likelihood Ratio test to the fitted model, it was found to be statistically significant $\because \ \alpha_{cal} << 0.05$ as shown below:

```{r Q8lrtest2, echo=FALSE}
anova(Question8Model2_null,Question8Model2, test='LRT')
```


### References

1. Dobson, A.J. and Barnett, A.G., 2018. *An introduction to generalized linear models.* CRC press.

2. Gbur, E.E., Stroup, W.W., McCarter, K.S., Durham, S., Young, L.J., Christman, M., West, M. and Kramer, M., 2020. *Analysis of generalized linear mixed models in the agricultural and natural resources sciences* (Vol. 156). John Wiley & Sons.